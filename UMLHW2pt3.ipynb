{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cameron Jester\n",
        "UML HW 2 Part 3"
      ],
      "metadata": {
        "id": "ekNJMIL1cs_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) The gaussian 2-dim data on file 2gaussian.txt has been generated using a mixture of two Gaussians, each 2-dim, with the parameters below. Run the EM algorithm with random initial values to recover the parameters.\n",
        "\n",
        "mean_1 [3,3]); cov_1 = [[1,0],[0,3]]; n1=2000 points\n",
        "\n",
        "mean_2 =[7,4]; cov_2 = [[1,0.5],[0.5,1]]; ; n2=4000 points\n",
        "\n",
        "You should obtain a result visually like this (you dont necessarily have to plot it)"
      ],
      "metadata": {
        "id": "SJT6j-kVcvpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbLfDidfcybV",
        "outputId": "01d9c2f1-f8dd-491a-b1fa-2d52556aaa84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sKfV46oci_T"
      },
      "outputs": [],
      "source": [
        "filepath = '/content/drive/My Drive/archive-3/2gaussian.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "read into python, linestrip, split so we get each. append then can initialize the parameters. Can start with uniform, idenity matrix\n",
        "\n",
        "\n",
        "max = 100 tolerance 10-6, in e step calc responsibilties e norm , update means, weights, cov . log liklihood for conversiosn, return weights, cov, means"
      ],
      "metadata": {
        "id": "v3s5UqaCdIrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Weights: [0.335, 0.665]\n",
        "- Mean 1: [2.994, 3.052]\n",
        "- Mean 2: [7.013, 3.983]\n",
        "- Cov 1: [[1.010, 0.027], [0.027, 2.938]]\n",
        "- Cov 2: [[0.975, 0.497], [0.497, 1.001]]\n"
      ],
      "metadata": {
        "id": "uC_A5pukeXk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_spd_matrix\n",
        "\n",
        "def em(filepath, k=2, max_iter=100, tolerance=1e-6):\n",
        "    data = np.loadtxt(filepath) #found this bit of code, which doesn't require preprocessing and line strip and all of that\n",
        "    n, d = data.shape\n",
        "\n",
        "\n",
        "    w = np.ones(k) / k\n",
        "    means = [np.random.rand(d) * 10 for _ in range(k)] #just setting random initialization\n",
        "    covariances = [make_spd_matrix(d) for _ in range(k)] #just setting random initialization\n",
        "\n",
        "    log_likelihood_old = -np.inf #starting with lowest initialization so that it doesnt stop after the first iteration\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        responsibilities = np.zeros((n, k))  #respons. array set to 0's to start\n",
        "\n",
        "        #e-step\n",
        "        log_likelihood = 0\n",
        "        for i in range(k):\n",
        "            diff = data - means[i]\n",
        "            exponent = np.sum(diff ** 2, axis=1)  #just the euclidean dist.\n",
        "            coeff = w[i] / np.sqrt((2 * np.pi) ** d)  # Coefficient for Gaussian distribution\n",
        "            responsibilities[:, i] = coeff * np.exp(-0.5 * exponent)  # Responsibilities for cluster i\n",
        "            log_likelihood += np.sum(coeff * np.exp(-0.5 * exponent)) #changing the log liklihood which we're using to see when converges\n",
        "\n",
        "        responsibilities /= responsibilities.sum(axis=1, keepdims=True)  #just nromalizing to from the sum of all points in each Guassian\n",
        "\n",
        "        #m-step\n",
        "        w = responsibilities.sum(axis=0) / n #getting the prop for each Gaussian\n",
        "        means = np.dot(responsibilities.T, data) / responsibilities.sum(axis=0)[:, np.newaxis] #multiplying matrices to get the sum of each weight, [:, np.newaxis] this to use when doing the cov\n",
        "\n",
        "        covariances = []\n",
        "        for i in range(k):\n",
        "            diff = data - means[i] #getting spread\n",
        "            weighted_sum = (responsibilities[:, i][:, np.newaxis] * diff).T @ diff #multiplying respons. matrix with the diff distances and then multiplying the transpose of that to differences\n",
        "            covariances.append(weighted_sum / responsibilities[:, i].sum()) #normalizing\n",
        "\n",
        "\n",
        "        #here is where we check for convergence, and stop once tolerance has been hit\n",
        "        if np.abs(log_likelihood - log_likelihood_old) < tolerance:\n",
        "            break\n",
        "        log_likelihood_old = log_likelihood\n",
        "\n",
        "    return w, means, covariances\n",
        "\n",
        "\n",
        "filepath = '/content/drive/My Drive/archive-3/2gaussian.txt'\n",
        "\n",
        "w, means, covariances = em(filepath)\n",
        "\n",
        "print(\"Weights:\", w)\n",
        "print(\"Means:\", means)\n",
        "print(\"Covariances\", covariances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRvIpGsxkbPB",
        "outputId": "bf5912b8-eeaf-42ca-8f62-b241e332b42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.65855342 0.34144658]\n",
            "Final Means: [[7.02628483 4.04463446]\n",
            " [3.04707894 2.9516151 ]]\n",
            "Final Covariances: [array([[0.97512989, 0.36194519],\n",
            "       [0.36194519, 0.98727766]]), array([[ 1.11665861, -0.12592307],\n",
            "       [-0.12592307,  2.70546119]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepathb = '/content/drive/My Drive/archive-3/3gaussian.txt'"
      ],
      "metadata": {
        "id": "nCbewKG-zBzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Literally the only thing I have changed between the two is just the k = 3, couldv'e just called the em def but\n",
        "# helped to look through everything to see if other chages were needed\n",
        "\n",
        "\n",
        "\n",
        "def em(filepath, k=3, max_iter=100, tolerance=1e-6):\n",
        "    data = np.loadtxt(filepath) #found this bit of code, which doesn't require preprocessing and line strip and all of that\n",
        "    n, d = data.shape\n",
        "\n",
        "\n",
        "    w = np.ones(k) / k\n",
        "    means = [np.random.rand(d) * 10 for _ in range(k)] #just setting random initialization\n",
        "    covariances = [make_spd_matrix(d) for _ in range(k)] #just setting random initialization\n",
        "\n",
        "    log_likelihood_old = -np.inf #starting with lowest initialization so that it doesnt stop after the first iteration\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        responsibilities = np.zeros((n, k))  #respons. array set to 0's to start\n",
        "\n",
        "        #e-step\n",
        "        log_likelihood = 0\n",
        "        for i in range(k):\n",
        "            diff = data - means[i]\n",
        "            exponent = np.sum(diff ** 2, axis=1)  #just the euclidean dist.\n",
        "            coeff = w[i] / np.sqrt((2 * np.pi) ** d)  # Coefficient for Gaussian distribution\n",
        "            responsibilities[:, i] = coeff * np.exp(-0.5 * exponent)  # Responsibilities for cluster i\n",
        "            log_likelihood += np.sum(coeff * np.exp(-0.5 * exponent)) #changing the log liklihood which we're using to see when converges\n",
        "\n",
        "        responsibilities /= responsibilities.sum(axis=1, keepdims=True)  #just nromalizing to from the sum of all points in each Guassian\n",
        "\n",
        "        #m-step\n",
        "        w = responsibilities.sum(axis=0) / n #getting the prop for each Gaussian\n",
        "        means = np.dot(responsibilities.T, data) / responsibilities.sum(axis=0)[:, np.newaxis] #multiplying matrices to get the sum of each weight, [:, np.newaxis] this to use when doing the cov\n",
        "\n",
        "        covariances = []\n",
        "        for i in range(k):\n",
        "            diff = data - means[i] #getting spread\n",
        "            weighted_sum = (responsibilities[:, i][:, np.newaxis] * diff).T @ diff #multiplying respons. matrix with the diff distances and then multiplying the transpose of that to differences\n",
        "            covariances.append(weighted_sum / responsibilities[:, i].sum()) #normalizing\n",
        "\n",
        "\n",
        "        #here is where we check for convergence, and stop once tolerance has been hit\n",
        "        if np.abs(log_likelihood - log_likelihood_old) < tolerance:\n",
        "            break\n",
        "        log_likelihood_old = log_likelihood\n",
        "\n",
        "    return w, means, covariances\n",
        "\n",
        "\n",
        "filepathb = '/content/drive/My Drive/archive-3/2gaussian.txt'\n",
        "\n",
        "w, means, covariances = em(filepathb) #would've done k = 3 if didn't reprint whole function\n",
        "\n",
        "print(\"Weights:\", w)\n",
        "print(\"Means:\", means)\n",
        "print(\"Covariances\", covariances)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEv9hOS9zorx",
        "outputId": "7a6f53f7-23e0-4383-dea9-194aa5fd23f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [0.40251005 0.27342487 0.32406508]\n",
            "Means: [[7.37500217 4.39064183]\n",
            " [6.37409567 3.44481782]\n",
            " [2.9507956  2.96931225]]\n",
            " [array([[0.83043945, 0.22116674],\n",
            "       [0.22116674, 0.80059555]]), array([[0.84939065, 0.16124943],\n",
            "       [0.16124943, 0.8633567 ]]), array([[ 0.97387339, -0.08766421],\n",
            "       [-0.08766421,  2.78423669]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Weights: [0.2, 0.3, 0.5] (2000/10000, 3000/10000, 5000/10000)\n",
        "- Mean 1: [3, 3]\n",
        "- Mean 2: [7, 4]\n",
        "- Mean 3: [5, 7]\n",
        "- Cov 1: [[1, 0], [0, 3]]\n",
        "- Cov 2: [[1, 0.5], [0.5, 1]]\n",
        "- Cov 3: [[1, 0.2], [0.2, 1]]"
      ],
      "metadata": {
        "id": "Cbmd_XXc0hv9"
      }
    }
  ]
}